{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables for API keys (e.g., OpenAI API key)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(r'C:\\Users\\DELL\\OneDrive\\Desktop\\chatbot\\env')  # Path to your .env file\n",
    "\n",
    "# Import necessary classes from LangChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "\n",
    "# Initialize OpenAI model for general LLM interactions\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.9)\n",
    "\n",
    "# Create a more interactive model for chat-based interactions\n",
    "llm_chat = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.0)  # Lower temperature for consistent responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Conversation History (Full): Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew.\n"
     ]
    }
   ],
   "source": [
    "## using ConversationBufferMemory\n",
    "\n",
    "# Create ConversationBufferMemory, which stores the entire conversation history\n",
    "memory_full = ConversationBufferMemory()\n",
    "\n",
    "# Set up a conversation chain with the memory that stores the entire history\n",
    "conversation_full = ConversationChain(\n",
    "    llm=llm_chat, \n",
    "    memory=memory_full,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation_full.predict(input=\"Hi, my name is Andrew\")  # First interaction\n",
    "conversation_full.predict(input=\"What is 1+1?\")  # Second interaction\n",
    "conversation_full.predict(input=\"What is my name?\")  # Should remember \"Andrew\"\n",
    "\n",
    "# Print the complete conversation history\n",
    "print(\"Conversation History (Full):\", memory_full.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Window Memory (Last 2 messages): {'history': \"Human: How are you?\\nAI: I'm good, thanks for asking!\\nHuman: What's the weather today?\\nAI: It's sunny and warm.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I do not have the capability to remember personal information like names. Is there anything else you would like to ask me?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using ConversationBufferWindowMemory\n",
    "\n",
    "#  Create a ConversationBufferWindowMemory with a window of the last 2 messages\n",
    "memory_window = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# Manually save interactions to test the buffer window\n",
    "memory_window.save_context({\"input\": \"Hello\"}, {\"output\": \"Hi there!\"})\n",
    "memory_window.save_context({\"input\": \"How are you?\"}, {\"output\": \"I'm good, thanks for asking!\"})\n",
    "memory_window.save_context({\"input\": \"What's the weather today?\"}, {\"output\": \"It's sunny and warm.\"})\n",
    "\n",
    "# Only the last two interactions should be saved due to k=2\n",
    "print(\"Conversation Window Memory (Last 2 messages):\", memory_window.load_memory_variables({}))\n",
    "\n",
    "# Create a conversation chain with a window memory that only remembers the last 1 message\n",
    "memory_window_1 = ConversationBufferWindowMemory(k=1)\n",
    "conversation_window_1 = ConversationChain(\n",
    "    llm=llm_chat, \n",
    "    memory=memory_window_1,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Simulate a conversation with a 1-message memory window\n",
    "conversation_window_1.predict(input=\"Hello, I'm Sam\")\n",
    "conversation_window_1.predict(input=\"What's 3 + 3?\")\n",
    "conversation_window_1.predict(input=\"Do you remember my name?\")  # Should not remember \"Sam\" due to k=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_20028\\817754504.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory_token = ConversationTokenBufferMemory(llm=llm_chat, max_token_limit=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Token Buffer Memory (Limited by 50 tokens): {'history': 'AI: It is a subset of AI.\\nHuman: Tell me about deep learning.\\nAI: It is a type of machine learning using neural networks.'}\n",
      "Token-Limited Chat Memory: AI: It is a subset of AI.\n",
      "Human: Tell me about deep learning.\n",
      "AI: It is a type of machine learning using neural networks.\n"
     ]
    }
   ],
   "source": [
    "## using ConversationTokenBufferMemory\n",
    "\n",
    "# Create a ConversationTokenBufferMemory that limits memory based on token count\n",
    "memory_token = ConversationTokenBufferMemory(llm=llm_chat, max_token_limit=50)\n",
    "\n",
    "# Manually add context to the memory while monitoring token usage\n",
    "memory_token.save_context({\"input\": \"Define AI\"}, {\"output\": \"AI stands for Artificial Intelligence.\"})\n",
    "memory_token.save_context({\"input\": \"What is machine learning?\"}, {\"output\": \"It is a subset of AI.\"})\n",
    "memory_token.save_context({\"input\": \"Tell me about deep learning.\"}, {\"output\": \"It is a type of machine learning using neural networks.\"})\n",
    "\n",
    "# Display the token-limited memory content\n",
    "print(\"Conversation Token Buffer Memory (Limited by 50 tokens):\", memory_token.load_memory_variables({}))\n",
    "\n",
    "# Display chat memory, which shows only content fitting within the token limit\n",
    "print(\"Token-Limited Chat Memory:\", memory_token.chat_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_20028\\396784343.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryMemory(llm=llm_chat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I'm Alex. Nice to meet you!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Alex introduces himself to the AI, and the AI responds by explaining its purpose as an assistant for any questions or tasks.\n",
      "Human: Can you tell me what the weather is like today?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Alex introduces himself to the AI, and the AI responds by explaining its purpose as an assistant for any questions or tasks. The human asks the AI to provide the current weather information, and the AI agrees to access the latest weather data for the location.\n",
      "Human: What do you know about quantum mechanics?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Alex introduces himself to the AI, and the AI explains its purpose as an assistant for any questions or tasks. The human asks the AI to provide the current weather information, and the AI agrees to access the latest weather data for the location. The human then inquires about quantum mechanics, and the AI provides a detailed explanation of the field, including its principles and technological applications.\n",
      "Human: Can you summarize what we’ve talked about so far?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Conversation Summary: {'history': 'Alex introduces himself to the AI, and the AI explains its purpose as an assistant for any questions or tasks. The human asks the AI to provide the current weather information, and the AI agrees to access the latest weather data for the location. The human then inquires about quantum mechanics, and the AI provides a detailed explanation of the field, including its principles and technological applications. The AI summarizes the conversation so far, offering to answer any more questions or discuss further topics.'}\n"
     ]
    }
   ],
   "source": [
    "## using ConversationSummaryMemory\n",
    "\n",
    "# Create a ConversationSummaryMemory to summarize conversation history\n",
    "summary_memory = ConversationSummaryMemory(llm=llm_chat)\n",
    "\n",
    "# Set up a conversation chain with the summarizing memory\n",
    "conversation_summary = ConversationChain(\n",
    "    llm=llm_chat, \n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation_summary.predict(input=\"Hi, I'm Alex. Nice to meet you!\")\n",
    "conversation_summary.predict(input=\"Can you tell me what the weather is like today?\")\n",
    "conversation_summary.predict(input=\"What do you know about quantum mechanics?\")\n",
    "conversation_summary.predict(input=\"Can you summarize what we’ve talked about so far?\")  # Should provide a summary\n",
    "\n",
    "# Print the current summary of the conversation\n",
    "print(\"Conversation Summary:\", summary_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
